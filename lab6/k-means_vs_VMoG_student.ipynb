{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: K-Means vs. Variational Mixture of Gaussians (VMoG)\n",
    "\n",
    "In this lab, you will explore two clustering algorithms:\n",
    "- **K-Means Clustering**: A hard clustering method that assigns each point to a single cluster.\n",
    "- **Variational Mixture of Gaussians (VMoG)**: A probabilistic clustering method that models soft cluster assignments.\n",
    "\n",
    "We will use the **Wine dataset** and compare how these methods cluster different types of wine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine Dataset üç∑\n",
    "<div>\n",
    "<img src=\"image.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "The Wine dataset is a classic machine learning dataset that contains chemical analyses of wines grown in the same region in Italy but derived from three different cultivars. The dataset includes 13 features:\n",
    "\n",
    "1. Alcohol content üç∑ (alcohol percentage)\n",
    "2. Malic acid üçé (tart, crisp taste)\n",
    "3. Ash üß™ (mineral content)\n",
    "4. Alcalinity of ash ‚öóÔ∏è (pH balance)\n",
    "5. Magnesium üî¨ (mineral content)\n",
    "6. Total phenols üåø (antioxidants)\n",
    "7. Flavanoids üçá (color compounds)\n",
    "8. Nonflavanoid phenols üå± (other phenols)\n",
    "9. Proanthocyanins üé® (color pigments)\n",
    "10. Color intensity üéØ (wine color)\n",
    "11. Hue üåà (color shade)\n",
    "12. OD280/OD315 of diluted wines üìä (protein content)\n",
    "13. Proline ‚öõÔ∏è (amino acid)\n",
    "\n",
    "The dataset consists of 178 samples, with each sample belonging to one of three different wine classes: Barolo, Grignolino, and Barbera.\n",
    "\n",
    "#### Barolo üëë - King of Wines\n",
    "- Higher alcohol content üç∑\n",
    "- Higher levels of phenols and flavanoids üåø\n",
    "- Deep, rich color intensity üé®\n",
    "- High proline content ‚öõÔ∏è\n",
    "\n",
    "#### Grignolino üå∫ - Light and Aromatic Red\n",
    "- Medium alcohol content üç∑\n",
    "- Higher acidity (malic acid) üçé\n",
    "- Lighter color intensity üé®\n",
    "- Lower phenol content üåø\n",
    "\n",
    "#### Barbera üçá - Bold, High-Acid Red Wine\n",
    "- Medium-high alcohol content üç∑\n",
    "- High acidity üçé\n",
    "- Deep color intensity üé®\n",
    "- Lower tannins (phenols) than Barolo üåø\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import multivariate_normal, mode\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "# Set visualization style\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess the Wine Dataset\n",
    "data = load_wine()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target  # True wine class labels\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the dataset for clustering\n",
    "scaler1 = StandardScaler()\n",
    "X_scaled = scaler1.fit_transform(df.drop(columns=['target']))\n",
    "scaler2 = MinMaxScaler()\n",
    "X_scaled_2 = scaler2.fit_transform(df.drop(columns=['target']))\n",
    "X_unscaled = df.drop(columns=['target']).values\n",
    "\n",
    "# Apply PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "df_pca['target'] = df['target']\n",
    "\n",
    "# Define class labels\n",
    "class_labels = {0: 'Barolo', 1: 'Grignolino', 2: 'Barbera'}\n",
    "df_pca['class'] = df_pca['target'].map(class_labels)\n",
    "\n",
    "# Visualize PCA projection\n",
    "plt.figure(figsize=(8, 6))\n",
    "#sns.scatterplot(x='PC1', y='PC2', hue='class', palette='rainbow', data=df_pca, alpha=0.7)\n",
    "sns.scatterplot(data=df_pca, x='PC1', y='PC2', \n",
    "                    hue='class', style='class', palette='Set1',\n",
    "                    markers={'Barolo': 'o', 'Grignolino': 's', 'Barbera': 'v'},\n",
    "                    alpha=0.7, s=80)\n",
    "plt.title('PCA Projection of Wine Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend(title='Wine Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: K-Means Clustering\n",
    "K-Means is a **hard clustering** algorithm that assigns each data point to one of $K$ clusters by minimizing intra-cluster variance. It can be derived as a special case of Variational Mixture of Gaussians (VMoG) by introducing a temperature parameter $\\beta$:\n",
    "\n",
    "$$\n",
    "    \\alpha_k^n = \\frac{\\exp(-\\frac{1}{2\\beta} || x^n - \\mu_k ||^2)}{\\sum_{k'} \\exp(-\\frac{1}{2\\beta} || x^n - \\mu_{k'} ||^2)}.\n",
    "$$\n",
    "\n",
    "When $\\beta \\to 0$, the posterior collapses to a hard assignment:\n",
    "\n",
    "$$\n",
    "    \\hat{z}^n = \\arg\\min_k || x^n - \\mu_k ||^2.\n",
    "$$\n",
    "\n",
    "The mean update simplifies to computing the centroid of assigned points:\n",
    "\n",
    "$$\n",
    "    \\mu_k = \\frac{1}{|S_k|} \\sum_{x^n \\in S_k} x^n,\n",
    "$$\n",
    "\n",
    "where $S_k$ is the set of points assigned to cluster $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KMeansFromScratch:\n",
    "    def __init__(self, n_clusters=3, max_iter=100, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    def initialize_centroids(self, X):\n",
    "        \"\"\"Initialize centroids randomly from data points\"\"\"\n",
    "        n_samples = X.shape[0] # X.shape = (178,13)\n",
    "        indices = np.random.choice(n_samples, self.n_clusters, replace=False) # indices.shape = (3,)\n",
    "        return X[indices] # X[indices].shape = (3,13)\n",
    "    \n",
    "    def assign_clusters(self, X, centroids):\n",
    "        \"\"\"Assign each point to nearest centroid\"\"\"\n",
    "        distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2)) # broadcast (1,178,13)-(3,1,13) -> (3,178,13); distances.shape = (3,178)\n",
    "        return np.argmin(distances, axis=0) # (178,)\n",
    "    \n",
    "    def update_centroids(self, X, labels):\n",
    "        \"\"\"Update centroids based on mean of assigned points\"\"\"\n",
    "        centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        for k in range(self.n_clusters):\n",
    "            if np.sum(labels == k) > 0:  # avoid empty clusters\n",
    "                centroids[k] = np.mean(X[labels == k], axis=0)\n",
    "        return centroids\n",
    "    \n",
    "    def compute_inertia(self, X, labels, centroids):\n",
    "        \"\"\"Compute sum of squared distances to nearest centroids\"\"\"\n",
    "        distances = np.sqrt(((X - centroids[labels])**2).sum(axis=1)) # centroids[labels].shape = (178,13), distances.shape = (178,)\n",
    "        return np.sum(distances**2)\n",
    "    \n",
    "    def fit(self, X, verbose=False):\n",
    "        \"\"\"Fit K-means to data\"\"\"\n",
    "        self.centroids_ = self.initialize_centroids(X)\n",
    "        self.inertia_history_ = []\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            # TODO: Assign points to clusters: labels\n",
    "            labels =\n",
    "            \n",
    "            # Store old centroids to check convergence\n",
    "            old_centroids = self.centroids_.copy()\n",
    "            \n",
    "            # TODO: Update centroids: self.centroids_\n",
    "            self.centroids_ =\n",
    "            \n",
    "            # Compute inertia\n",
    "            inertia = self.compute_inertia(X, labels, self.centroids_)\n",
    "            self.inertia_history_.append(inertia)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Iteration {iteration}: Inertia = {inertia:.4f}\")\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.allclose(old_centroids, self.centroids_):\n",
    "                break\n",
    "                \n",
    "        self.labels_ = labels\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"K-Means Fitting Process:\")\n",
    "kmeans = KMeansFromScratch(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X_scaled, verbose=True)\n",
    "\n",
    "# Q: what about fitting onto X_unscaled or X_scaled_2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "def map_clusters_to_labels(true_labels, cluster_labels):\n",
    "    \"\"\"\n",
    "    Maps cluster labels to true labels by maximizing matching frequencies.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_labels : array-like\n",
    "        True class labels\n",
    "    cluster_labels : array-like\n",
    "        Predicted cluster labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Mapping from cluster labels to true labels\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    remaining_labels = set(np.unique(true_labels))\n",
    "    \n",
    "    # For each cluster, find the most common true label\n",
    "    for cluster in unique_clusters:\n",
    "        mask = (cluster_labels == cluster)\n",
    "        if len(remaining_labels) > 0:\n",
    "            # Find the most common true label in this cluster\n",
    "            true_label_mode, count = mode(true_labels[mask], keepdims=False)\n",
    "            \n",
    "            # If this label is still available, map to it\n",
    "            if true_label_mode in remaining_labels:\n",
    "                mapping[cluster] = true_label_mode\n",
    "                remaining_labels.remove(true_label_mode)\n",
    "            else:\n",
    "                # If already taken, map to any remaining label\n",
    "                mapping[cluster] = remaining_labels.pop()\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "\n",
    "def plot_clustering_with_accuracy(data, cluster_col, title=None):\n",
    "    \"\"\"\n",
    "    Plot clustering results showing correct/incorrect classifications with automatic cluster mapping.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame\n",
    "        DataFrame containing 'PC1', 'PC2', 'class', 'target' and clustering results\n",
    "    cluster_col : str\n",
    "        Name of the column containing cluster assignments\n",
    "    title : str, optional\n",
    "        Plot title. If None, will be generated based on cluster_col\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Create a copy of the data to avoid modifying the original\n",
    "    plot_data = data.copy()\n",
    "    \n",
    "    # Map cluster numbers to true classes\n",
    "    cluster_mapping = map_clusters_to_labels(\n",
    "        plot_data['target'].values,\n",
    "        plot_data[cluster_col].values\n",
    "    )\n",
    "    \n",
    "    # Apply mapping to get aligned cluster labels\n",
    "    plot_data['aligned_clusters'] = plot_data[cluster_col].map(cluster_mapping)\n",
    "    \n",
    "    # Determine correct classifications using aligned clusters\n",
    "    plot_data['correct'] = (plot_data['aligned_clusters'] == plot_data['target']).map(\n",
    "        {True: 'Correct', False: 'Incorrect'}\n",
    "    )\n",
    "    \n",
    "    # Create the scatter plot\n",
    "    sns.scatterplot(data=plot_data, x='PC1', y='PC2', \n",
    "                    hue='correct', style='class',\n",
    "                    markers={'Barolo': 'o', 'Grignolino': 's', 'Barbera': 'v'},\n",
    "                    palette={'Correct': 'green', 'Incorrect': 'red'},\n",
    "                    alpha=0.7, s=80)\n",
    "    \n",
    "    # Set title\n",
    "    if title is None:\n",
    "        title = f'{cluster_col} Results'\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    \n",
    "    # Modify legend\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles, labels,\n",
    "              title='Wine Class',\n",
    "              bbox_to_anchor=(1.05, 1),\n",
    "              loc='upper left')\n",
    "    \n",
    "    # Display cluster mapping\n",
    "    mapping_text = \"Cluster mapping:\\n\" + \"\\n\".join(\n",
    "        f\"Cluster {k} ‚Üí Class {v}\" for k, v in cluster_mapping.items()\n",
    "    )\n",
    "    plt.text(0.02, 0.85, mapping_text,\n",
    "             transform=plt.gca().transAxes,\n",
    "             bbox=dict(facecolor='white', alpha=0.8),\n",
    "             verticalalignment='top',\n",
    "             fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return cluster_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca['KMeans_Cluster'] = kmeans.labels_\n",
    "# Visualize K-Means Clusters\n",
    "plot_clustering_with_accuracy(df_pca, 'KMeans_Cluster', 'K-Means Clustering Results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Evaluation Metrics\n",
    "\n",
    "We use three different metrics to evaluate our clustering results:\n",
    "\n",
    "### 1. Silhouette Score\n",
    "The Silhouette Score measures how similar a point is to its own cluster compared to other clusters. Range: [-1, 1]\n",
    "- **Score = 1**: Perfect clustering (points are far from neighboring clusters)\n",
    "- **Score = 0**: Points are on or close to decision boundary\n",
    "- **Score = -1**: Points might be assigned to wrong clusters\n",
    "\n",
    "### 2. Adjusted Mutual Information (AMI) Score\n",
    "AMI measures the mutual information between true labels and predicted clusters, adjusted for chance. Range: [0, 1]\n",
    "- **Score = 1**: Perfect alignment with true labels\n",
    "- **Score = 0**: Labels are completely independent\n",
    "- Adjustment for chance ensures random labelings score close to 0\n",
    "\n",
    "### 3. Adjusted Rand Index (ARI)\n",
    "ARI measures the similarity between two clusterings, adjusted for chance agreement. Range: [-1, 1]\n",
    "- **Score ‚âà 1**: Strong agreement with true labels\n",
    "- **Score ‚âà 0**: Clusterings are as good as random\n",
    "- **Score < 0**: Less agreement than expected by chance\n",
    "\n",
    "\n",
    "### Key Differences\n",
    "- **Silhouette Score**: Measures cluster quality based on geometry, doesn't need true labels\n",
    "- **AMI**: Measures information shared between clusterings, accounting for label permutations\n",
    "- **ARI**: Measures pair-wise agreement between clusterings, more sensitive to number of clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Clustering Results\n",
    "from sklearn.metrics import adjusted_mutual_info_score, adjusted_rand_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def evaluate_clustering(true_labels, predicted_labels, method_name):\n",
    "    \"\"\"\n",
    "    Evaluate clustering results using multiple metrics\n",
    "    \"\"\"\n",
    "    ami = adjusted_mutual_info_score(true_labels, predicted_labels)\n",
    "    ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "    silhouette = silhouette_score(X_scaled, df_pca['KMeans_Cluster'])\n",
    "    \n",
    "    print(f\"\\n{method_name} Clustering Evaluation:\")\n",
    "    print(f\"Adjusted Mutual Information Score: {ami:.4f}\")\n",
    "    print(f\"Adjusted Rand Score: {ari:.4f}\")\n",
    "    print(f'Silhouette Score: {silhouette:.4f}')\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {method_name}')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_clustering(df['target'], df_pca['KMeans_Cluster'], 'K-Means')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Variational Mixture of Gaussians (VMoG)\n",
    "VMoG is a probabilistic clustering model where each data point has a soft assignment to multiple Gaussian components. Given latent variable $z$ representing the component assignment, the generative process follows:\n",
    "\n",
    "prior: $ p(z) = \\frac{1}{M} $, likelihood (generative distribution): $p(x | z=k) = \\mathcal{N}(x | \\mu_k, \\Sigma_k), \\Sigma_k = I$\n",
    "\n",
    "Objective function:\n",
    "$$ J = \\frac{1}{N} \\sum_{n=1}^N (\\sum_{m=1}^M \\alpha_m^n ( -\\frac{1}{2} \\| x^n - \\mu_m \\|^2- \\frac{d}{2} \\log 2\\pi) + \\sum_{m=1}^M \\alpha_m^n  \\log M - \\sum_{m=1}^M \\alpha_m^n \\log \\alpha_m ^ n) $$\n",
    "\n",
    "The solution to the approximate posterior for $z$:\n",
    "$$ q(z=k; \\phi_n) = \\alpha_k^n = \\frac{\\exp\\left(-\\frac{1}{2} \\| x^n - \\mu_k \\|^2\\right)}{\\sum_{k'=1}^K \\exp\\left(-\\frac{1}{2} \\| x^n - \\mu_{k'} \\|^2\\right)} $$\n",
    "\n",
    "The solution to the mean $\\mu_k$:\n",
    "$$ \\mu_k = \\sum_{n=1}^N \\frac{\\alpha_k^n}{\\sum_{n'=1}^N \\alpha_k^{n'}} x^n $$\n",
    "\n",
    "### **Initialization**\n",
    "We often use K-Means centroids to initialize $\\mu_k, \\Sigma_k, p(z)$.\n",
    "\n",
    "### **Expectation-Maximization (E-M) Algorithm**\n",
    "The E-M algorithm iteratively updates $\\alpha_k^n$ and $\\mu_k$ by maximizing $J$\n",
    "\n",
    "#### **1. Expectation Step (E-Step)**\n",
    "Computes the responsibility $\\alpha_k^n$.\n",
    "\n",
    "#### **2. Maximization Step (M-Step)**\n",
    "Updates the parameters $\\mu_k$, covariances ($\\Sigma_k$), and weights ($p(z = k)$) to maximize the lower bound:\n",
    "\n",
    "**Updating Covariances:**\n",
    "$$ \\Sigma_k = \\frac{\\sum_n \\alpha_k^n (x_n - \\mu_k)(x_n - \\mu_k)^T}{\\sum_n \\alpha_k^n} $$\n",
    "\n",
    "**Updating Weights:**\n",
    "$$ p(z = k) = \\frac{\\sum_n \\alpha_k^n}{N} $$\n",
    "\n",
    "The model stops updating when the lower bound stops increasing.\n",
    "\n",
    "Unlike K-Means, VMoG accounts for uncertainty in cluster assignments, making it more flexible for overlapping or elliptical clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VMoGFromScratch:\n",
    "    def __init__(self, n_components=3, max_iter=100, random_state=None):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    def initialize_parameters(self, X, kmeans=None):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if kmeans is not None:\n",
    "            # Use custom K-Means results for initialization\n",
    "            self.means_ = kmeans.centroids_ # from KMeansFromScratch\n",
    "            labels = kmeans.labels_ # from KMeansFromScratch\n",
    "\n",
    "            self.covs_ = np.zeros((self.n_components, n_features, n_features))\n",
    "            self.weights_ = np.zeros(self.n_components)\n",
    "\n",
    "            for k in range(self.n_components):\n",
    "                cluster_indices = np.where(labels == k)[0]  # Corrected\n",
    "                cluster_points = X[cluster_indices]\n",
    "                n_points = len(cluster_points)\n",
    "\n",
    "                if n_points > 0:\n",
    "                    diff = cluster_points - self.means_[k]\n",
    "                    self.covs_[k] = np.dot(diff.T, diff) / n_points\n",
    "                    self.covs_[k] += 1e-6 * np.eye(n_features)  # Regularization\n",
    "                    self.weights_[k] = n_points / n_samples\n",
    "                else:\n",
    "                    self.covs_[k] = np.eye(n_features)  # Fallback\n",
    "                    self.weights_[k] = 1e-6\n",
    "\n",
    "            self.weights_ /= np.sum(self.weights_)  # Normalize\n",
    "\n",
    "        else:\n",
    "            indices = np.random.choice(n_samples, self.n_components, replace=False)\n",
    "            self.means_ = X[indices]\n",
    "            self.covs_ = np.array([np.eye(n_features) for _ in range(self.n_components)])\n",
    "            self.weights_ = np.ones(self.n_components) / self.n_components\n",
    "\n",
    "    def e_step(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        log_resp = np.zeros((n_samples, self.n_components))\n",
    "\n",
    "        for k in range(self.n_components):\n",
    "            try:\n",
    "                log_likelihood = multivariate_normal.logpdf(\n",
    "                    X, mean=self.means_[k], cov=self.covs_[k] + 1e-6 * np.eye(X.shape[1])\n",
    "                )\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(f\"Warning: Singular covariance for component {k}. Resetting.\")\n",
    "                self.covs_[k] += 1e-6 * np.eye(X.shape[1])\n",
    "                log_likelihood = multivariate_normal.logpdf(X, mean=self.means_[k], cov=self.covs_[k])\n",
    "\n",
    "            log_resp[:, k] = np.log(np.maximum(self.weights_[k], 1e-10)) + log_likelihood\n",
    "\n",
    "        # Use stable softmax computation to normalize responsibilities\n",
    "        log_resp -= logsumexp(log_resp, axis=1, keepdims=True)\n",
    "        resp = np.exp(log_resp)\n",
    "\n",
    "        return resp\n",
    "\n",
    "    def m_step(self, X, resp):\n",
    "        n_samples = X.shape[0]\n",
    "        nk = resp.sum(axis=0) + 1e-10  # Avoid division by zero\n",
    "        # using Bayesian prior to prevent weights from collapsing to zero\n",
    "        alpha_prior = 1e-2\n",
    "        self.weights_ = (nk + alpha_prior) / (n_samples + self.n_components * alpha_prior)\n",
    "\n",
    "        # Q: what about not using a prior?\n",
    "        # hint: self.weights_ = nk / n_samples\n",
    "\n",
    "        self.means_ = np.dot(resp.T, X) / nk[:, np.newaxis]\n",
    "\n",
    "        for k in range(self.n_components):\n",
    "            diff = X - self.means_[k]\n",
    "            weighted_diff = diff.T @ np.diag(resp[:, k]) @ diff / nk[k]  # Corrected\n",
    "            self.covs_[k] = weighted_diff + 1e-3 * np.eye(X.shape[1])  # Regularization\n",
    "\n",
    "    def compute_lower_bound(self, X, resp):\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        log_likelihood = 0\n",
    "        for k in range(self.n_components):\n",
    "            log_likelihood += np.sum(resp[:, k] * (\n",
    "                np.log(np.maximum(self.weights_[k], 1e-10)) + \n",
    "                multivariate_normal.logpdf(X, self.means_[k], self.covs_[k] + 1e-6 * np.eye(X.shape[1]))\n",
    "            ))\n",
    "\n",
    "        entropy = -np.sum(resp * np.log(resp + 1e-10))\n",
    "\n",
    "        return log_likelihood + entropy\n",
    "\n",
    "    def check_convergence(self, iteration):\n",
    "        if iteration < 2:\n",
    "            return False\n",
    "\n",
    "        current_lb = self.lower_bound_history_[-1]\n",
    "        previous_lb = self.lower_bound_history_[-2]\n",
    "\n",
    "        if np.isclose(previous_lb, current_lb):\n",
    "            return True\n",
    "\n",
    "        if current_lb < previous_lb:\n",
    "            print(f\"Warning: Lower bound decreased from {previous_lb:.4f} to {current_lb:.4f}\")\n",
    "\n",
    "        return False\n",
    "\n",
    "    def fit(self, X, verbose=False, kmeans=None):\n",
    "        self.initialize_parameters(X, kmeans)\n",
    "        self.lower_bound_history_ = []\n",
    "        self.n_iter_ = 0\n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "            # TODO: update responsibilities with E-step\n",
    "            resp =\n",
    "            # TODO: M-step\n",
    "            \n",
    "            # TODO: compute J\n",
    "            lower_bound =\n",
    "            \n",
    "            self.lower_bound_history_.append(lower_bound)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Iteration {iteration}: Lower bound = {lower_bound:.4f}\")\n",
    "\n",
    "            if self.check_convergence(iteration):\n",
    "                if verbose:\n",
    "                    print(f\"Converged after {iteration + 1} iterations\")\n",
    "                self.n_iter_ = iteration + 1\n",
    "                break\n",
    "\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"Did not converge after {self.max_iter} iterations\")\n",
    "            self.n_iter_ = self.max_iter\n",
    "\n",
    "        self.responsibilities_ = resp\n",
    "        self.labels_ = resp.argmax(axis=1)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nVMoG Fitting Process:\")\n",
    "vmog = VMoGFromScratch(n_components=3, max_iter=100, random_state=42).fit(X_scaled, kmeans=kmeans, verbose=True)\n",
    "# Q: what about starting from random initialization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca['VMoG_Cluster'] = vmog.labels_\n",
    "\n",
    "# Visualize VMoG Clusters\n",
    "vmog_mapping = plot_clustering_with_accuracy(df_pca, 'VMoG_Cluster', 'VMoG Classification Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate VMoG\n",
    "evaluate_clustering(df['target'], df_pca['VMoG_Cluster'], 'VMoG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show the fitting process\n",
    "leave some blank spaces\n",
    "hyperparameter sweep: number of clusters\n",
    "how is it related to variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sklean K-Means Clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "df_pca['KMeans_Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Visualize K-Means Clusters\n",
    "plot_clustering_with_accuracy(df_pca, 'KMeans_Cluster', 'K-Means Clustering Results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Sklearn BayesianGaussianMixture\n",
    "vmog = BayesianGaussianMixture(n_components=3, random_state=42)\n",
    "df_pca['VMoG_Cluster'] = vmog.fit_predict(X_scaled)\n",
    "\n",
    "# Visualize VMoG Clusters\n",
    "vmog_mapping = plot_clustering_with_accuracy(df_pca, 'VMoG_Cluster', 'VMoG Classification Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate K-Means\n",
    "evaluate_clustering(df['target'], df_pca['KMeans_Cluster'], 'K-Means')\n",
    "\n",
    "# Evaluate VMoG\n",
    "evaluate_clustering(df['target'], df_pca['VMoG_Cluster'], 'VMoG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the Two Methods\n",
    "- Which method assigns clusters more closely to the true wine classes?\n",
    "- Does VMoG provide **better separation** than K-Means?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
