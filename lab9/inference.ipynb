{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9\n",
    "\n",
    "In the previous lab, we had a posterior distibution we couldn't sample from directly: $p(z | x_L)$, where $x_L$ was part of an image. We then constructed a sampler for this posterior using MCMC. \n",
    "\n",
    "In this lab, we will study transformer autoregressive models, which allows for sampling by construction. Let $x$ be the input to the model and $y_{1:T}$ be the output. An autoregressive model learns the distribution $p(y|x) = \\prod_{i=1}^T p(y_i | y_{<i}, x)$. Here, we use the chain rule to predict the $y_i$'s one at a time, thereby modeling the joint distribution. For more on the transformer, read this [blog post](https://medium.com/@zepingyu/123-cb62513f5d50).\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "In the case of language models, the $y_i$'s are tokens. To process text using a language model, we need to first convert the text into tokens, or chunks of characters (subwords). One can technically set the $y_i$'s to characters, and character-level language models indeed exist, but they are not as efficient as token-level language models when using a standard attention scheme. The most common tokenization scheme is Byte Pair Encoding (BPE), which runs like this:\n",
    "1. Initialize the vocabulary with characters\n",
    "2. Count the frequency of adjacent token pairs in the corpus\n",
    "3. Merge the most frequent pair to create a new token\n",
    "4. Add this new token to the vocabulary\n",
    "5. Repeat steps 2-4 until a target vocabulary size is reached\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forced choice\n",
    "\n",
    "blimp = load_dataset(\"nyu-mll/blimp\", \"causative\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence_good': 'Aaron breaks the glass.',\n",
       " 'sentence_bad': 'Aaron appeared the glass.',\n",
       " 'field': 'syntax',\n",
       " 'linguistics_term': 'argument_structure',\n",
       " 'UID': 'causative',\n",
       " 'simple_LM_method': True,\n",
       " 'one_prefix_method': False,\n",
       " 'two_prefix_method': False,\n",
       " 'lexically_identical': False,\n",
       " 'pair_id': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blimp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_blimp_grammaticality(tokenizer, model, blimp, phenomenon=None):\n",
    "    \"\"\"\n",
    "    Loads the BLiMP dataset from Hugging Face,\n",
    "    performs forced choice grammaticality judgment using a language model,\n",
    "    and reports overall accuracy.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: The tokenizer to use for encoding sentences\n",
    "        model: The language model to use for scoring sentences\n",
    "        phenomena: List of specific phenomena to evaluate. If None, evaluates all phenomena.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing overall accuracy and per-phenomenon results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = {\n",
    "    }\n",
    "    \n",
    "    # Make sure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    phenomenon_correct = 0\n",
    "    phenomenon_total = len(blimp)\n",
    "    \n",
    "    # Track scores for this phenomenon\n",
    "    good_scores = []\n",
    "    bad_scores = []\n",
    "    \n",
    "    # Iterate through each minimal pair in this phenomenon\n",
    "    for item in tqdm(blimp, desc=f\"Evaluating {phenomenon}\", leave=False):\n",
    "        # Get the grammatical and ungrammatical sentences\n",
    "        grammatical = item[\"sentence_good\"]\n",
    "        ungrammatical = item[\"sentence_bad\"]\n",
    "        \n",
    "        # Compute log likelihood for grammatical sentence\n",
    "        grammatical_score = compute_log_likelihood(grammatical, tokenizer, model)\n",
    "        \n",
    "        # Compute log likelihood for ungrammatical sentence\n",
    "        ungrammatical_score = compute_log_likelihood(ungrammatical, tokenizer, model)\n",
    "        \n",
    "        # Store scores for analysis\n",
    "        good_scores.append(grammatical_score)\n",
    "        bad_scores.append(ungrammatical_score)\n",
    "        \n",
    "        # Prediction is correct if grammatical sentence has higher log likelihood\n",
    "        prediction_correct = grammatical_score > ungrammatical_score\n",
    "        \n",
    "        # Update counter\n",
    "        if prediction_correct:\n",
    "            phenomenon_correct += 1\n",
    "    \n",
    "    # Calculate accuracy for this phenomenon\n",
    "    phenomenon_accuracy = phenomenon_correct / phenomenon_total\n",
    "    \n",
    "    # Store results for this phenomenon\n",
    "    results[phenomenon] = {\n",
    "        \"correct\": phenomenon_correct,\n",
    "        \"total\": phenomenon_total,\n",
    "        \"accuracy\": phenomenon_accuracy,\n",
    "        \"avg_good_score\": np.mean(good_scores),\n",
    "        \"avg_bad_score\": np.mean(bad_scores),\n",
    "        \"avg_score_diff\": np.mean(np.array(good_scores) - np.array(bad_scores))\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compute_log_likelihood(sentence, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Compute the log likelihood of a sentence using the provided model.\n",
    "    \n",
    "    Args:\n",
    "        sentence: The sentence to compute log likelihood for\n",
    "        tokenizer: The tokenizer to use for encoding\n",
    "        model: The language model\n",
    "        \n",
    "    Returns:\n",
    "        float: The log likelihood of the sentence\n",
    "    \"\"\"\n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get the input IDs for easier reference\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    \n",
    "    # We'll compute the average log likelihood per token\n",
    "    with torch.no_grad():\n",
    "        # Get the model outputs\n",
    "        outputs = model(**inputs, labels=input_ids)\n",
    "        \n",
    "        # Get the loss (negative log likelihood)\n",
    "        neg_log_likelihood = outputs.loss.item()\n",
    "        \n",
    "        # Convert to positive log likelihood (higher is better)\n",
    "        log_likelihood = -neg_log_likelihood\n",
    "    \n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    }
   ],
   "source": [
    "results = evaluate_blimp_grammaticality(tokenizer, model, blimp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'does ethanol take more energy make that produces', 'answer': False, 'passage': \"All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3270/3270 [00:00<00:00, 7370.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# constrained decoding\n",
    "\n",
    "boolq = load_dataset(\"google/boolq\", split=\"validation\")\n",
    "print(boolq[0])\n",
    "\n",
    "def merge_passage_and_question(example):\n",
    "    input_text = example[\"passage\"] + \" \" + example[\"question\"] + \"? \"\n",
    "    return {\"input_text\": input_text}\n",
    "\n",
    "boolq = boolq.map(merge_passage_and_question, remove_columns=[\"passage\", \"question\"])\n",
    "\n",
    "# tokenize\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"input_text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_boolq = boolq.map(tokenize_function, batched=True, remove_columns=[\"input_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2787, 4245]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels = ['True', 'False']\n",
    "valid_token_ids = [tokenizer.encode(label)[0] for label in valid_labels]\n",
    "label2token = {\n",
    "    True: tokenizer.encode(\"True\")[0],\n",
    "    False: tokenizer.encode(\"False\")[0]\n",
    "}\n",
    "valid_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def constrained_decoding(model, dataset, valid_token_ids):\n",
    "    \"\"\"\n",
    "    Performs constrained decoding using a language model on a pre-tokenized dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model to use for scoring.\n",
    "        dataset: A pre-tokenized dataset containing input_ids and labels.\n",
    "        valid_token_ids: Tensor of token IDs that are allowed as next tokens.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation results and predictions.\n",
    "    \"\"\"\n",
    "    # Ensure valid_token_ids is a tensor on the right device\n",
    "    if not isinstance(valid_token_ids, torch.Tensor):\n",
    "        valid_token_ids = torch.tensor(valid_token_ids, device=model.device)\n",
    "    else:\n",
    "        valid_token_ids = valid_token_ids.to(model.device)\n",
    "    \n",
    "    # Results dictionary\n",
    "    results = {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0,\n",
    "        \"scores\": [],\n",
    "    }\n",
    "    \n",
    "    # Process all examples in the dataset\n",
    "    for item in tqdm(dataset, desc=\"Performing constrained decoding\"):\n",
    "        # Get input ids and move to the same device as the model\n",
    "        input_ids = torch.tensor(item[\"input_ids\"]).to(model.device)\n",
    "        attention_mask = torch.tensor(item[\"attention_mask\"]).to(model.device)\n",
    "        true_label = item[\"answer\"]\n",
    "        true_label_token = label2token[true_label]\n",
    "\n",
    "        \n",
    "        # Ensure input is batched (add batch dimension if needed)\n",
    "        if input_ids.dim() == 1:\n",
    "            input_ids = input_ids.unsqueeze(0)\n",
    "            attention_mask = attention_mask.unsqueeze(0)\n",
    "        \n",
    "        # Forward pass to get logits\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get logits for the last token position\n",
    "        last_token_logits = logits[0, -1, :]\n",
    "        \n",
    "        # Extract logits only for valid tokens using torch.take_along_dim\n",
    "        valid_token_logits = torch.take_along_dim(\n",
    "            last_token_logits.unsqueeze(0),\n",
    "            valid_token_ids.unsqueeze(0),\n",
    "            dim=1\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        # Find the highest scoring valid token\n",
    "        max_logit_idx = torch.argmax(valid_token_logits).item()\n",
    "        scores = F.softmax(valid_token_logits, dim=-1)\n",
    "        score = scores[max_logit_idx].item()\n",
    "        label_token = valid_token_ids[max_logit_idx].cpu().item()\n",
    "\n",
    "        results[\"scores\"].append(score)\n",
    "        results[\"total\"] += 1\n",
    "        results[\"correct\"] += (label_token == true_label_token)\n",
    "\n",
    "    return results   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Performing constrained decoding: 100%|██████████| 10/10 [00:55<00:00,  5.59s/it]\n"
     ]
    }
   ],
   "source": [
    "results = constrained_decoding(model, tokenized_boolq.select(range(10)), valid_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
