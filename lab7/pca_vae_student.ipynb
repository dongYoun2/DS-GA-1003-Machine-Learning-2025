{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Principal Component Analysis (PCA) vs. Variatioinal Autoencoders (VAEs)\n",
    "\n",
    "In this lab, we will explore PCA and VAEs.\n",
    "- **PCA**: Unsupervised dimensionality reduction technique.\n",
    "- **VAEs**: A generative model with continuous and probabilistic latent representation.\n",
    "\n",
    "We’ll use the **FashionMNIST** dataset.\n",
    "\n",
    "- Dataset loading and overview\n",
    "- PCA\n",
    "- VAEs\n",
    "- Conditional VAEs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FashionMNIST dataset\n",
    "<div>\n",
    "<img src=\"image.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Required Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(28),        # Resize images to 28x28 for consistency\n",
    "    transforms.Grayscale(num_output_channels=1),  # Ensure 1 channel (grayscale)\n",
    "    transforms.ToTensor(),        # Convert to tensor (values in [0,1])\n",
    "])\n",
    "\n",
    "# Load FashionMNIST dataset\n",
    "train_dataset = datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.FashionMNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# Define DataLoader for training\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Print dataset size\n",
    "print(f\"Training set size: {len(train_dataset)}, Test set size: {len(test_dataset)}\")\n",
    "\n",
    "# Define FashionMNIST class names\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "print(class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some sample images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_sample_images(dataset, num_samples=8):\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(12, 2))\n",
    "    for i in range(num_samples):\n",
    "        img, label = dataset[i]\n",
    "        axes[i].imshow(img.squeeze(0), cmap=\"gray\")\n",
    "        axes[i].axis(\"off\")\n",
    "        # Add class name as title above each image\n",
    "        axes[i].set_title(class_names[label], fontsize=12)\n",
    "    plt.tight_layout()  # Adjust layout to prevent title overlap\n",
    "    plt.show()\n",
    "\n",
    "# Show sample images from the dataset\n",
    "show_sample_images(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA recap\n",
    "\n",
    "## 1. PCA Definitions: variance maximization\n",
    "\n",
    "PCA seeks a unit vector $w$ that maximizes the variance of the projected data: $\\max_{\\|w\\|=1} \\text{Var}(w^\\top x)$\n",
    "$$\\text{Var}(w^\\top x) = \\mathbb{E}[(w^\\top x−\\mathbb{E}[w^\\top x])^2] = \\mathbb{E}[(w^\\top (x−\\mathbb{E}[x])) (w^\\top (x−\\mathbb{E}[x])^\\top] = w^\\top \\mathbb{E}[(x - \\mathbb{E} [x])(x - \\mathbb{E} [x])^\\top] w = w^\\top \\Sigma_x w$$\n",
    "$\\Sigma_x$ is the covariance matrix of data $x$: $\\Sigma_x = \\mathbb{E}[(x - \\mathbb{E} [x])(x - \\mathbb{E} [x])^\\top]$\n",
    "\n",
    "The solution to $\\max_{\\|w\\|=1} (w^\\top \\Sigma_x w)$ is the eigenvector of $\\Sigma_x$: $w_1$, with the largest eigenvalue: $\\lambda_1$: $w_1^\\top \\Sigma_x w_1 = \\lambda_1$\n",
    "\n",
    "All of the eigenvectors (solution) make a matrix $W$ such that:\n",
    "$$W^\\top \\Sigma_x W = \\Lambda$$\n",
    "where:\n",
    "  - $W$ contains eigenvectors of $\\Sigma_x$.\n",
    "  - $\\Lambda$ is a diagonal matrix of eigenvalues (principal variances).\n",
    "\n",
    "Then we will project data $x$ onto the principal component directions:\n",
    "$$z = W^{-1} (x - \\mathbb{E} [x])$$\n",
    "\n",
    "\n",
    "## 2. Connection to variational inference\n",
    "PCA can be derived as a special case of a probabilistic generative model where:\n",
    "- The latent variable $z$ follows a Gaussian prior:\n",
    "$p(z) = N(z; 0, \\sigma^2 I^{|z|})$\n",
    "- The observed data $x$ is modeled as a linear transformation of $z$ plus Gaussian noise:\n",
    "$p(x|z) = N(x; Wz + b, I^{|x|})$\n",
    "- The approximate posterior: $q(z; \\phi(x_n)) = \\mathcal{N}(z; \\mu_n, I^{|z|})$\n",
    "\n",
    "To solve the objective function $J = \\frac{1}{N} \\sum_n J_n$, we get analytical solution to $\\mu_n$:\n",
    "$$\\mu_n = (W^{\\top}W + \\sigma^{-2}I)^{-1}W^{\\top}(x_n - b)$$\n",
    "\n",
    "**Connection to PCA**.\n",
    "As $\\sigma^2 \\to \\infty$, meaning there is no prior information:\n",
    "$$\\mu_n = (W^{\\top}W)^{-1}W^{\\top}(x_n - b)$$\n",
    "If $W$ is square, invertible:\n",
    "$$\\mu_n = W^{-1}(x_n - b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Implementation\n",
    "\n",
    "There are different ways to implement PCA.\n",
    "\n",
    "### Eigenvalue decomposition (EVD) of covariance matrix:\n",
    "\n",
    "1. Prepare data and standardize (zero-mean): $N$ samples with dimension of $D$\n",
    "2. Compute covariance matrix:\n",
    "   - Calculate the covariance matrix $C = \\frac{1}{N-1} X^T X$ (since data is zero-mean)\n",
    "   - $C$ will be a $D \\times D$ matrix\n",
    "3. Eigen-decomposition:\n",
    "   - Compute the eigenvalues and eigenvectors of $C$\n",
    "   - The eigenvectors corresponding to the largest eigenvalues are the principal components\n",
    "4. Dimensionality reduction:\n",
    "   - To project onto the top-$k$ components, take the first $k$ eigenvectors\n",
    "   - Transform $X$ to a reduced representation $Z = X W_k$\n",
    "   - Reconstruct approximately as $\\hat{X} = Z W_k^T$ (adding back the mean)\n",
    "\n",
    "### Singular value decomposition (SVD)\n",
    "\n",
    "Instead of computing the covariance matrix, apply singular value decomposition (SVD) to the centered data matrix $X$\n",
    "$$X = USV^\\top$$\n",
    "where:\n",
    "- $U$ contains the left singular vectors (not used in PCA).\n",
    "- $S$ is a diagonal matrix of singular values, which are eigenvalues of covariance matrix.\n",
    "- $V$ contains the right singular vectors, which correspond to eigenvectors of covariance matrix.\n",
    "\n",
    "SVD is more numerically stable than EVD, and avoids explicit computation of covariance matrix, making it better for high-dimensional data and large datasets.\n",
    "\n",
    "### Expectation-maximization (EM) algorithm\n",
    "\n",
    "Using the connection to variational inference, we can use the EM algorithm to find $W$ and $b$.\n",
    "$$p(x|z) = N(x; Wz + b, I^{|x|})$$\n",
    "EM Algorithm Steps:\n",
    "- E-step: Compute the posterior mean of the latent variable\n",
    "$$\\mu_n = (W^{\\top}W + \\sigma^{-2}I)^{-1}W^{\\top}(x_n - b)$$\n",
    "- M-step: Update $W$ and $b$\n",
    "$$W = \n",
    "    \\left(\\frac{1}{N} \\sum_{n=1}^N (x_n - b)\\mu_n^\\top \\right)\n",
    "    \\left(I + \\frac{1}{N}\\sum_{n=1}^N \\mu_n \\mu_n^\\top\\right)^{-1}$$\n",
    "$$b = \\frac{1}{N} \\sum_{n=1}^N \\left( W \\mu_n - x_n\\right)$$\n",
    "- Repeat until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation using EVD\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Select a subset of the FashionMNIST dataset for PCA (e.g., first 1000 images)\n",
    "pca_subset_size = 1000\n",
    "pca_indices = list(range(pca_subset_size))\n",
    "pca_subset = Subset(train_dataset, pca_indices)\n",
    "\n",
    "# Flatten images and create data matrix X\n",
    "X_list = []\n",
    "for img, _ in pca_subset:\n",
    "    X_list.append(img.view(-1))  # Flatten image (28x28 -> 784)\n",
    "X = torch.stack(X_list)  # Shape: (1000, 784)\n",
    "X = X.to(device)  # Move to GPU for faster computation\n",
    "\n",
    "# Step 1: Standardize the data (zero mean)\n",
    "mean_vector = X.mean(dim=0)  # Compute mean for each pixel across all images\n",
    "X_centered = X - mean_vector  # Subtract mean\n",
    "\n",
    "# Step 2: Compute covariance matrix\n",
    "N, D = X_centered.shape\n",
    "cov_matrix = (X_centered.T @ X_centered) / (N - 1)  # (D x D)\n",
    "\n",
    "# Step 3: Eigen-decomposition of covariance matrix\n",
    "eig_vals, eig_vecs = torch.linalg.eigh(cov_matrix)  # Eigenvalues in ascending order\n",
    "\n",
    "# Step 4: Sort eigenvalues and eigenvectors in descending order\n",
    "eig_vals, indices = torch.sort(eig_vals, descending=True)\n",
    "eig_vecs = eig_vecs[:, indices]  # Sorted eigenvectors\n",
    "\n",
    "print(\"Top 5 eigenvalues:\", eig_vals[:5])\n",
    "\n",
    "# Step 5: Project onto the first k principal components\n",
    "k = 50  # Number of principal components to retain\n",
    "Wk = eig_vecs[:, :k]  # Top-k eigenvectors (D x k)\n",
    "Z = X_centered @ Wk  # Projected data (1000, k)\n",
    "\n",
    "# Reconstruct images from the reduced space\n",
    "X_reconstruct = Z @ Wk.T + mean_vector  # Approximate original images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "# Move data back to CPU for visualization\n",
    "mean_vector_cpu = mean_vector.cpu()\n",
    "eig_vecs_cpu = eig_vecs.cpu()\n",
    "X_centered_cpu = X_centered.cpu()\n",
    "\n",
    "# Compute and display the mean image\n",
    "mean_img = mean_vector_cpu.reshape(28, 28).numpy()\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.title(\"Mean Image\")\n",
    "plt.imshow(mean_img, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# Display the first 5 principal components (eigenimages)\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10,2))\n",
    "for i in range(5):\n",
    "    pc_img = eig_vecs_cpu[:, i].reshape(28, 28).numpy()\n",
    "    axes[i].imshow(pc_img, cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f\"PC {i+1}\", fontsize=12)\n",
    "plt.suptitle(\"First 5 Principal Components\", fontsize=12, y=1.1)\n",
    "plt.show()\n",
    "\n",
    "# Choose a sample image and reconstruct it using top-k components\n",
    "sample_idx = 0  # First image in the subset\n",
    "x_original = X[sample_idx] + mean_vector  # Add mean back\n",
    "z_sample = torch.matmul(X_centered[sample_idx], Wk)  # Project onto top-k components\n",
    "x_reconstruct = torch.matmul(z_sample, Wk.T) + mean_vector  # Reconstruct\n",
    "\n",
    "# Reshape for visualization\n",
    "x_original_img = x_original.reshape(28, 28).cpu().numpy()\n",
    "x_reconstruct_img = x_reconstruct.reshape(28, 28).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(4,2))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(x_original_img, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(f\"Reconstructed (k={k})\")\n",
    "plt.imshow(x_reconstruct_img, cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAEs recap\n",
    "\n",
    "### PCA limitation.\n",
    "\n",
    "PCA assumes linearity and models only Gaussian-distributed data, making it unsuitable for capturing complex, nonlinear structures in real-world datasets.\n",
    "\n",
    "### VAEs is a nonlinear extension of PCA.\n",
    "\n",
    "VAEs introduce nonlinear mappings through neural networks:\n",
    "$$p(x|z) = N(x|F(z;\\theta), I)$$\n",
    "where $F(z;\\theta)$ is a neural network that learns complex, nonlinear relationships.\n",
    "\n",
    "### Key Components.\n",
    "\n",
    "- Latent variable model $G(x)$.\n",
    "    \n",
    "    Since we do not have an analytic solution to $\\mu_n$ in $q(z; \\phi(x_n)) = \\mathcal{N}(z; \\mu_n, I^{|z|})$, we must store and update the $\\mu_n$ for every training sample during Monte Carlo sampling, which becomes infeasible for large datasets.\n",
    "    \n",
    "    Instead, we use a neural network $G(x)$ (encoder): \n",
    "    $$\\mu_n = G(x_n;\\theta_G)$$\n",
    "    - Avoid large storage requirement\n",
    "    - Enables generalization\n",
    "\n",
    "- Reparametrization trick.\n",
    "\n",
    "    We need to do sampling during training: $z \\sim q(z)$, but We cannot backpropagate through the sampling operation because it is a non-differentiable operation.\n",
    "    To enable gradient computation through sampling, VAEs employ:\n",
    "    $$z = \\mu + \\sigma\\epsilon$$\n",
    "    where $\\epsilon \\sim N(0, I)$.\n",
    "    \n",
    "    This allows gradients to flow through the latent variables during training.\n",
    "\n",
    "- Loss Function.\n",
    "    \n",
    "    VAEs optimize the variational lower bound consisting of two terms:\n",
    "    $$J_n = \\mathbb{E}_{z \\sim q_n(z; G(x_n; \\theta_G), \\sigma^2)} \n",
    "    \\left[ -\\frac{1}{2} \\| x_n - F(z;\\theta) \\|^2 - \\frac{|x|}{2} \\log 2\\pi\\right] - \\frac{1}{2} \\left[ \\frac{K + \\|G(x_n; \\theta_G)\\|^2}{\\sigma^2} - K + 2K \\ln(\\sigma) \\right]$$\n",
    "    A single sample estimate:\n",
    "    $$\\tilde{J}_n = -\\frac{1}{2}\\| x_n - F(G(x_n; \\theta_G)+\\sigma \\epsilon; \\theta) \\|^2 - \\frac{1}{2 \\sigma^2} \\| G(x_n; \\theta_G)\\|^2 + \\mathrm{const.}$$\n",
    "    where:\n",
    "    - First term: Reconstruction loss\n",
    "    - Second term: KL divergence loss: minimizing L2 norm - ensuring the space of the latent variable $z$, as tightly as possible. Otherwise, sampling $z \\sim p(z)$ would result in invalid or meaningless prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_size = 28 * 28  # FashionMNIST images are 28x28\n",
    "\n",
    "        # Encoder: Fully connected layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),  # Flatten image to 784\n",
    "            nn.Linear(self.img_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2 * latent_dim)  # Outputs mu and logvar concatenated\n",
    "        )\n",
    "\n",
    "        # Decoder: Fully connected layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.img_size),\n",
    "            nn.Sigmoid()  # Sigmoid to output values in [0,1] for BCE loss\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encodes input images into latent mean and log-variance vectors.\"\"\"\n",
    "        h = self.encoder(x)  # (batch, 2*latent_dim)\n",
    "        mu = h[:, :self.latent_dim]\n",
    "        logvar = h[:, self.latent_dim:]\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Samples z from N(mu, sigma^2) using the reparameterization trick.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)  # Compute standard deviation\n",
    "        eps = torch.randn_like(std)  # Sample epsilon ~ N(0, I)\n",
    "        z = mu + eps * std  # Reparameterization trick\n",
    "        return z\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Decodes latent vector z to reconstructed image.\"\"\"\n",
    "        x_reconst_flat = self.decoder(z)  # Output is flattened image\n",
    "        x_reconst = x_reconst_flat.view(-1, 1, 28, 28)  # Reshape to (batch, 1, 28, 28)\n",
    "        return x_reconst\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO\n",
    "        \n",
    "        return x_reconst, mu, logvar\n",
    "\n",
    "# Instantiate the VAE model\n",
    "latent_dim = 20  # Adjustable latent dimension\n",
    "vae = VAE(latent_dim).to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "\n",
    "print(f\"VAE model initialized with latent dimension: {latent_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    \"\"\"Computes the VAE loss: Reconstruction loss + KL divergence\"\"\"\n",
    "    # Reconstruction loss (BCE)\n",
    "    recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum') / x.size(0)\n",
    "    \n",
    "    # KL divergence loss (per batch average)\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)\n",
    "    \n",
    "    return recon_loss + kl_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FashionMNIST dataset\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "vae.train()\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    for images, _ in train_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "        loss = loss_function(recon_images, images, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to evaluation mode\n",
    "vae.eval()\n",
    "\n",
    "# Load a few test images\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "images, _ = next(iter(test_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "# Get reconstructions\n",
    "with torch.no_grad():\n",
    "    recon_images, _, _ = vae(images)\n",
    "\n",
    "# Move to CPU for visualization\n",
    "images = images.cpu()\n",
    "recon_images = recon_images.cpu()\n",
    "\n",
    "# Plot original vs reconstructed\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
    "for i in range(8):\n",
    "    axes[0, i].imshow(images[i].squeeze(0), cmap='gray')\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(recon_images[i].squeeze(0), cmap='gray')\n",
    "    axes[1, i].axis('off')\n",
    "axes[0,0].set_title(\"Original\")\n",
    "axes[1,0].set_title(\"Reconstructed\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new images by sampling random latent vectors\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(8, latent_dim).to(device)  # 8 random latent vectors\n",
    "    gen_images = vae.decode(z).cpu()\n",
    "\n",
    "# Plot generated images\n",
    "fig, axes = plt.subplots(1, 8, figsize=(12, 2))\n",
    "for i in range(8):\n",
    "    axes[i].imshow(gen_images[i].squeeze(0).numpy(), cmap='gray')\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle(\"Generated Images from Random z\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional VAEs\n",
    "\n",
    "To extend VAE to a conditional VAE (CVAE), we need to condition both the encoder and decoder on the label, $c$.\n",
    "\n",
    "- Modify the encoder: Instead of encoding only the image $x$, also encode the label $c$.\n",
    "\n",
    "- Modify the decoder: Instead of decoding only the latent vector $z$, also pass the label $c$ to generate a conditioned image.\n",
    "\n",
    "- Modify the reparameterization trick: The sampling process remains the same, but $z$ now depends on both $x$ and $c$.\n",
    "\n",
    "- Modify the forward pass: Ensure that labels $c$ are passed through the model correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional VAEs\n",
    "# label c concatenate to image, also sending to q\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define the Conditional VAE model\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=20, num_classes=10):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.img_size = 28 * 28  # FashionMNIST images are 28x28\n",
    "        self.num_classes = num_classes  # Number of labels\n",
    "\n",
    "        # One-hot embedding for labels\n",
    "        self.label_embedding = nn.Embedding(num_classes, num_classes)\n",
    "\n",
    "        #TODO\n",
    "        # Encoder: Fully connected layers with conditioning\n",
    "        \n",
    "        #TODO\n",
    "        # Decoder: Fully connected layers with conditioning\n",
    "        \n",
    "\n",
    "    def encode(self, x, c):\n",
    "        \"\"\"Encodes input images and labels into latent mean and log-variance vectors.\"\"\"\n",
    "        c_embed = self.label_embedding(c)  # Convert labels to embeddings\n",
    "        c_embed = c_embed.view(c.shape[0], -1)  # Reshape to match batch size\n",
    "        x = torch.cat([x.view(x.shape[0], -1), c_embed], dim=1)  # Concatenate\n",
    "        h = self.encoder(x)  # (batch, 2*latent_dim)\n",
    "        mu = h[:, :self.latent_dim]\n",
    "        logvar = h[:, self.latent_dim:]\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Samples z from N(mu, sigma^2) using the reparameterization trick.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)  # Compute standard deviation\n",
    "        eps = torch.randn_like(std)  # Sample epsilon ~ N(0, I)\n",
    "        z = mu + eps * std  # Reparameterization trick\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, c):\n",
    "        \"\"\"Decodes latent vector z and label c to reconstructed image.\"\"\"\n",
    "        c_embed = self.label_embedding(c)  # Convert labels to embeddings\n",
    "        c_embed = c_embed.view(c.shape[0], -1)\n",
    "        z = torch.cat([z, c_embed], dim=1)  # Concatenate latent vector and label\n",
    "        x_reconst_flat = self.decoder(z)  # Output is flattened image\n",
    "        x_reconst = x_reconst_flat.view(-1, 1, 28, 28)  # Reshape to (batch, 1, 28, 28)\n",
    "        return x_reconst\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        #TODO\n",
    "        \n",
    "        return x_reconst, mu, logvar\n",
    "\n",
    "# Instantiate the CVAE model\n",
    "latent_dim = 20  # Adjustable latent dimension\n",
    "num_classes = 10  # Number of classes in FashionMNIST\n",
    "cvae = CVAE(latent_dim, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=1e-3)\n",
    "\n",
    "print(f\"CVAE model initialized with latent dimension: {latent_dim}, and {num_classes} classes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FashionMNIST dataset\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "cvae.train()\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    total_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        recon_images, mu, logvar = cvae(images, labels)\n",
    "        loss = loss_function(recon_images, images, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set model to evaluation mode\n",
    "cvae.eval()\n",
    "\n",
    "# Sample a random latent vector\n",
    "z_sample = torch.randn(1, latent_dim).to(device)\n",
    "\n",
    "# Choose a label (e.g., class 3)\n",
    "label = torch.tensor([2]).to(device)\n",
    "\n",
    "# Generate an image\n",
    "with torch.no_grad():\n",
    "    generated_image = cvae.decode(z_sample, label).cpu()\n",
    "\n",
    "# Plot the generated image\n",
    "plt.imshow(generated_image.squeeze(0).squeeze(0), cmap=\"gray\")\n",
    "plt.title(f\"Generated Image for Label {label.item()}, {class_names[label.item()]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
